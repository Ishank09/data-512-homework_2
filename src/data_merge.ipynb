{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "021edb89",
   "metadata": {},
   "source": [
    "# Introduction: Data Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c04768",
   "metadata": {},
   "source": [
    "This Jupyter Notebook outlines a data processing pipeline for merging multiple datasets related to Wikipedia articles, US cities, and US states by region. The ultimate goal is to create a comprehensive dataset containing information about the regional divisions, populations, Wikipedia article details, and ORES predictions for various US states.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc9989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b553df0",
   "metadata": {},
   "source": [
    "# Step 1: Reading the datasets\n",
    "In this step, we read three separate datasets: 'cleaned_data.csv', 'us_cities_by_state_SEPT.2023.csv', and 'US States by Region - US Census Bureau.xlsx'. These datasets contain crucial information about ORES predictions, US city details, and US state divisions, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da721f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading the 'ores_predictions.csv' file\n",
    "ores_df = pd.read_csv('../data/cleaned_data.csv') \n",
    "\n",
    "## Reading the 'us_cities_by_state_SEPT.2023.csv' file\n",
    "cities_df = pd.read_csv('../data/us_cities_by_state_SEPT.2023.csv')\n",
    "\n",
    "## Reading the 'US States by Region - US Census Bureau.xlsx' file\n",
    "regions_df = pd.read_excel('../data/US States by Region - US Census Bureau.xlsx')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ab6a60",
   "metadata": {},
   "source": [
    "# Step 2: Data Preprocessing\n",
    "This step involves necessary data preprocessing tasks. We extract the 'State' column from the 'us_cities_by_state_SEPT.2023.csv' dataset. Additionally, we merge the 'ores_df' and 'cities_df' dataframes to combine relevant information from both datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cc1a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting the 'State' column from the cities dataframe\n",
    "cities_df = cities_df[['page_title', 'State']]\n",
    "\n",
    "## Merging the 'ores_df' and 'cities_df' on the 'Title' and 'page_title' columns\n",
    "merged_df = pd.merge(ores_df, cities_df, left_on='Title', right_on='page_title', how='inner')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54ebbc1",
   "metadata": {},
   "source": [
    "# Step 3: Merging the Dataframes\n",
    "Here, we merge the previously combined dataframe with the 'regions_df' dataframe based on the 'State' column. This results in a comprehensive dataframe containing data from all three initial datasets, providing a holistic view of the US states, their regions, and corresponding Wikipedia article details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaa93da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merging the 'merged_df' and 'regions_df' on the 'State' column\n",
    "final_df = pd.merge(merged_df, regions_df, left_on='State', right_on='STATE', how='inner')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb62095e",
   "metadata": {},
   "source": [
    "# Step 4: Selecting the Required Columns\n",
    "To streamline the dataset, we select only the necessary columns, including 'State', 'DIVISION', 'article_title', 'Last_Revision_ID', and 'Prediction'. This ensures that the resulting dataset remains focused on the essential information for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc4ce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Selecting the necessary columns for the final dataset\n",
    "final_df = final_df[['State', 'DIVISION', 'article_title', 'Last_Revision_ID', 'Prediction']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eddcb0c",
   "metadata": {},
   "source": [
    "# Step 5: Renaming the Columns\n",
    "To improve the readability of the final dataset, we rename the columns to more intuitive and descriptive names, providing a clearer understanding of the data contained within the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e22b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Renaming the columns for better readability\n",
    "final_df.columns = ['state', 'regional_division', 'article_title', 'revision_id', 'article_quality']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5aa4ae",
   "metadata": {},
   "source": [
    "# Step 6: Saving the Final Dataset\n",
    "Finally, we save the resulting dataset to a CSV file named 'resulting_data.csv'. This file contains all the essential information merged from the initial datasets, offering valuable insights into US state regional divisions, populations, Wikipedia article details, and ORES predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef814253",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the resulting data to a CSV file\n",
    "final_df.to_csv('../data/resulting_data.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the final merged dataset\n",
    "final_df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
