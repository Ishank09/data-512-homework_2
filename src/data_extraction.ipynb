{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa16c9bb",
   "metadata": {},
   "source": [
    "## Data Extraction\n",
    "This notebook presents a Python script for querying Wikipedia to gather page information for a dataset of U.S. cities by state. The script covers various key aspects to achieve this data retrieval task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94044ad",
   "metadata": {},
   "source": [
    "## Importing Essential Python Modules for Data Processing and HTTP Requests\n",
    "This section imports essential Python modules required for the subsequent code execution. It includes modules like json for handling JSON data, time for time-related operations, requests for making HTTP requests, pandas for data manipulation, and tqdm for creating progress bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f40ed38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary Python modules\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d69f45b",
   "metadata": {},
   "source": [
    "## Code Attribution and Licensing Information\n",
    "Original Code Attribution\n",
    "This code example was developed by Dr. David W. McDonald for use in DATA 512, a course in the UW MS Data Science degree program.\n",
    "This code is provided under the Creative Commons CC-BY license. Revision 1.1 - August 14, 2023\n",
    "Source: https://colab.research.google.com/drive/15UoE16s-IccCTOXREjU3xDIz07tlpyrl#scrollTo=2i0WSJn4TXqu&printMode=true\n",
    "\n",
    "Define Constants and Configuration\n",
    "\n",
    "*****This is modified version of the code.\n",
    "\n",
    "\n",
    "## Defining Constants and Configuration for Wikipedia API Requests\n",
    "This part defines various constants and configuration settings used throughout the code. It includes settings for the Wikipedia API endpoint, assumed API latency, and request headers. It also defines parameters for querying Wikipedia pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e53d2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "API_LATENCY_ASSUMED = 0.002\n",
    "API_THROTTLE_WAIT = (1.0/100.0) - API_LATENCY_ASSUMED\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<uwnetid@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2023',\n",
    "}\n",
    "PAGEINFO_EXTENDED_PROPERTIES = \"talkid|url|watched|watchers\"\n",
    "PAGEINFO_PARAMS_TEMPLATE = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"\",\n",
    "    \"prop\": \"info\",\n",
    "    \"inprop\": PAGEINFO_EXTENDED_PROPERTIES\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674e2a7e",
   "metadata": {},
   "source": [
    "## Defining a Function for Requesting Wikipedia Page Information\n",
    "This section defines a Python function named request_pageinfo_per_article. This function is used to request information about a single Wikipedia article. It checks for valid input and handles exceptions, ensuring that API requests are made correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb251f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to request page info for a single article\n",
    "def request_pageinfo_per_article(article_title=None, endpoint_url=API_ENWIKIPEDIA_ENDPOINT,\n",
    "                                 request_template=PAGEINFO_PARAMS_TEMPLATE, headers=REQUEST_HEADERS):\n",
    "    if article_title:\n",
    "        request_template['titles'] = article_title\n",
    "    if not request_template['titles']:\n",
    "        raise Exception(\"Must supply an article title to make a pageinfo request.\")\n",
    "\n",
    "    try:\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(endpoint_url, headers=headers, params=request_template)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d94f3f",
   "metadata": {},
   "source": [
    "## Loading U.S. City Data from CSV into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0377e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Wikipedia Data from CSV\n",
    "df = pd.read_csv('../data/us_cities_by_state_SEPT.2023.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3568c1eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>page_title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Abbeville, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abbeville,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Adamsville, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Adamsville,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Addison, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Addison,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Akron, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Akron,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Alabaster, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Alabaster,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22152</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Wamsutter, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wamsutter,_Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22153</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Wheatland, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wheatland,_Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22154</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Worland, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Worland,_Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22155</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Wright, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wright,_Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22156</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Yoder, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Yoder,_Wyoming</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22157 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         state           page_title   \n",
       "0      Alabama   Abbeville, Alabama  \\\n",
       "1      Alabama  Adamsville, Alabama   \n",
       "2      Alabama     Addison, Alabama   \n",
       "3      Alabama       Akron, Alabama   \n",
       "4      Alabama   Alabaster, Alabama   \n",
       "...        ...                  ...   \n",
       "22152  Wyoming   Wamsutter, Wyoming   \n",
       "22153  Wyoming   Wheatland, Wyoming   \n",
       "22154  Wyoming     Worland, Wyoming   \n",
       "22155  Wyoming      Wright, Wyoming   \n",
       "22156  Wyoming       Yoder, Wyoming   \n",
       "\n",
       "                                                     url  \n",
       "0       https://en.wikipedia.org/wiki/Abbeville,_Alabama  \n",
       "1      https://en.wikipedia.org/wiki/Adamsville,_Alabama  \n",
       "2         https://en.wikipedia.org/wiki/Addison,_Alabama  \n",
       "3           https://en.wikipedia.org/wiki/Akron,_Alabama  \n",
       "4       https://en.wikipedia.org/wiki/Alabaster,_Alabama  \n",
       "...                                                  ...  \n",
       "22152   https://en.wikipedia.org/wiki/Wamsutter,_Wyoming  \n",
       "22153   https://en.wikipedia.org/wiki/Wheatland,_Wyoming  \n",
       "22154     https://en.wikipedia.org/wiki/Worland,_Wyoming  \n",
       "22155      https://en.wikipedia.org/wiki/Wright,_Wyoming  \n",
       "22156       https://en.wikipedia.org/wiki/Yoder,_Wyoming  \n",
       "\n",
       "[22157 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f59d1e",
   "metadata": {},
   "source": [
    "# Requesting Wikipedia Page Info for U.S. Cities and Storing in CSV\n",
    "This code block initiates the process of querying Wikipedia for information about each city in the DataFrame. It iterates through the list of city titles, requests page information, and stores the results in a list. The list is then converted into a DataFrame, and the resulting data is saved to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94ddc2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22157/22157 [1:29:54<00:00,  4.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# Request and store Page Info for Wikipedia Articles in a CSV file\n",
    "data = []\n",
    "for title in tqdm(df['page_title'].tolist()):\n",
    "    info = request_pageinfo_per_article(title)\n",
    "    if 'query' in info and 'pages' in info['query']:\n",
    "        pages = info['query']['pages']\n",
    "        for key, value in pages.items():\n",
    "            if 'lastrevid' in value and 'title' in value:\n",
    "                data.append({'Title': value['title'], 'Last_Revision_ID': value['lastrevid']})\n",
    "\n",
    "# Convert the data into a DataFrame and store it in a CSV file\n",
    "result_df = pd.DataFrame(data)\n",
    "result_df.to_csv('../data/wiki_page_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f32cff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
